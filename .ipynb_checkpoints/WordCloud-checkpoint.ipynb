{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from fa2 import ForceAtlas2\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib import error, request\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from numba import jit\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create quote dictionary from filteredQuotes\n",
    "f=open(\"filteredQuotes.txt\",'r')\n",
    "characterQuotes={}\n",
    "r=f.read()\n",
    "text=r.split('\\n')\n",
    "for quote in text: \n",
    "    character=quote.split('splitsplitsplit')[0]\n",
    "    split_quote=quote.split()[1:]\n",
    "    try: \n",
    "        split_quote.remove('splitsplitsplit')\n",
    "    except:\n",
    "        pass\n",
    "    characters=character.split(' and ')\n",
    "    for person in characters:\n",
    "        person =person.strip()\n",
    "        if characterQuotes.get(person):\n",
    "                characterQuotes[person]+=split_quote\n",
    "        else:\n",
    "                characterQuotes[person]=split_quote\n",
    "f.close()\n",
    "\n",
    "#create documents for tf-idf\n",
    "documents=[]\n",
    "characterName=[]\n",
    "for person in characterQuotes.keys():\n",
    "    if len(characterQuotes[person])>400:\n",
    "        quotes=characterQuotes[person]\n",
    "        strQuotes=\" \".join(quotes)\n",
    "        tokens=word_tokenize(strQuotes.lower())\n",
    "        listToks=sorted(tokens)\n",
    "        documents.append(listToks)\n",
    "        characterName.append(person)\n",
    "\n",
    "documentsWithUniqueWords = [sorted(set(x)) for x in characterQuotes.values() if len(x)>400]\n",
    "documentsLength = len(documents)\n",
    "\n",
    "# Calculate the tf-idf of a word\n",
    "#@jit(cache=True)\n",
    "def tfIdf(word, text):\n",
    "    # Count how many documents include the word\n",
    "    count = 0\n",
    "    for document in documentsWithUniqueWords:\n",
    "        if word in document:\n",
    "            count += 1\n",
    "    # Calculate the idf of the word\n",
    "    idf = math.log( documentsLength / count)\n",
    "    if (text.count(word) / len(text)) * idf >1:\n",
    "        print(word+\" \"+(text.count(word) / len(text)) * idf)\n",
    "    return (text.count(word) / len(text)) * idf\n",
    "\n",
    "\n",
    "# Calculate and return a string in the way that the wordcloud library wants it, using the tf-idf algorithm\n",
    "#@jit(cache=True)\n",
    "def getWorldCloudStringFromADocument(document, documentWithUniqueWords):\n",
    "    finalWords = []\n",
    "    for word in documentWithUniqueWords:\n",
    "        try:\n",
    "            wordAppearance = tfIdf(word, document)*10000\n",
    "        except:\n",
    "            raise\n",
    "        if wordAppearance > 0:\n",
    "            for _ in range(round(wordAppearance)):\n",
    "                finalWords.append(word)\n",
    "            # finalWords.append(wordAppearance)\n",
    "    return finalWords\n",
    "\n",
    "# An array with all the strings for the wordcloud\n",
    "res = []\n",
    "resCharacterNames=[]\n",
    "for documentIndex in range(len(documents)):\n",
    "    if len(documents[documentIndex])!=0:\n",
    "        wordList = getWorldCloudStringFromADocument(documents[documentIndex], documentsWithUniqueWords[documentIndex])\n",
    "        res.append(wordList)\n",
    "        resCharacterNames.append(characterName[documentIndex])\n",
    "#set each list as a string for the word cloud\n",
    "res = [\" \".join(text) for text in res ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homer\n",
    "# homer_coloring = np.array(Image.open('pictures/Homer_Simpson.jpg'))\n",
    "# wc_homer = WordCloud(background_color=\"white\",\n",
    "#                      max_words=4000,\n",
    "#                      mask=homer_coloring,\n",
    "#                      max_font_size=100,\n",
    "#                      random_state=42,\n",
    "#                      contour_color=\"#000000\",\n",
    "#                      contour_width = 2,\n",
    "#                      collocations=False)\n",
    "\n",
    "# #Marge\n",
    "# marge_coloring = np.array(Image.open('pictures/Marge_Simpson.jpeg'))\n",
    "# wc_marge = WordCloud(background_color=\"white\",\n",
    "#                      max_words=4000,\n",
    "#                      mask=marge_coloring,\n",
    "#                      max_font_size=100,\n",
    "#                      collocations = False,\n",
    "#                      random_state=42)\n",
    "\n",
    "# #Bart\n",
    "# bart_coloring = np.array(Image.open('pictures/Bart_Simpson.jpg'))\n",
    "# wc_bart = WordCloud(background_color=\"white\",\n",
    "#                     max_words=4000,\n",
    "#                     mask=bart_coloring,\n",
    "#                     max_font_size=100,\n",
    "#                     collocations=False,\n",
    "#                     random_state=42)\n",
    "\n",
    "#Lisa\n",
    "lisa_coloring = np.array(Image.open('pictures/LisaTransparent.png'))\n",
    "wc_lisa = WordCloud(background_color=\"white\",\n",
    "                    max_words=4000,\n",
    "                    mask=lisa_coloring,\n",
    "                    max_font_size=100,\n",
    "                    collocations=False,\n",
    "                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1a1993e748>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wc_homer.generate(res[characterName.index('Homer')])\n",
    "# wc_marge.generate(res[characterName.index('Marge')])\n",
    "# wc_bart.generate(res[characterName.index('Bart')])\n",
    "wc_lisa.generate(res[characterName.index('Lisa')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58799\n",
      "60690\n",
      "47814\n",
      "52528\n"
     ]
    }
   ],
   "source": [
    "print(len(res[characterName.index('Lisa')]))\n",
    "print(len(res[characterName.index('Homer')]))\n",
    "print(len(res[characterName.index('Bart')]))\n",
    "print(len(res[characterName.index('Marge')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_colors_homer = ImageColorGenerator(homer_coloring)\n",
    "# image_colors_marge = ImageColorGenerator(marge_coloring)\n",
    "# image_colors_bart = ImageColorGenerator(bart_coloring)\n",
    "image_colors_lisa = ImageColorGenerator(lisa_coloring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(300,300))\n",
    "plt.imshow(wc_lisa.recolor(color_func=image_colors_lisa),interpolation=\"bilinear\")\n",
    "plt.savefig(\"LisaWc.png\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
